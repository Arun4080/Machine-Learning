{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFStdwWUbiEM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmlgpcGmdOlJ"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "hm_lines = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0mEM9dttz2l"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_lexicon(pos,neg):\n",
    "\n",
    "\tlexicon = []\n",
    "\twith open(pos,'r') as f:\n",
    "\t\tcontents = f.readlines()\n",
    "\t\tfor l in contents[:hm_lines]:\n",
    "\t\t\tall_words = word_tokenize(l)\n",
    "\t\t\tlexicon += list(all_words)\n",
    "\n",
    "\twith open(neg,'r') as f:\n",
    "\t\tcontents = f.readlines()\n",
    "\t\tfor l in contents[:hm_lines]:\n",
    "\t\t\tall_words = word_tokenize(l)\n",
    "\t\t\tlexicon += list(all_words)\n",
    "\n",
    "\tlexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "\tw_counts = Counter(lexicon)\n",
    "\tl2 = []\n",
    "\tfor w in w_counts:\n",
    "\t\t#print(w_counts[w])\n",
    "\t\tif 1000 > w_counts[w] > 50:\n",
    "\t\t\tl2.append(w)\n",
    "\tprint(len(l2))\n",
    "\treturn l2\n",
    "\n",
    "\n",
    "def sample_handling(sample,lexicon,classification):\n",
    "\n",
    "\tfeatureset = []\n",
    "\n",
    "\twith open(sample,'r') as f:\n",
    "\t\tcontents = f.readlines()\n",
    "\t\tfor l in contents[:hm_lines]:\n",
    "\t\t\tcurrent_words = word_tokenize(l.lower())\n",
    "\t\t\tcurrent_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "\t\t\tfeatures = np.zeros(len(lexicon))\n",
    "\t\t\tfor word in current_words:\n",
    "\t\t\t\tif word.lower() in lexicon:\n",
    "\t\t\t\t\tindex_value = lexicon.index(word.lower())\n",
    "\t\t\t\t\tfeatures[index_value] += 1\n",
    "\n",
    "\t\t\tfeatures = list(features)\n",
    "\t\t\tfeatureset.append([features,classification])\n",
    "\n",
    "\treturn featureset\n",
    "\n",
    "\n",
    "\n",
    "def create_feature_sets_and_labels(pos,neg,test_size = 0.1):\n",
    "\tlexicon = create_lexicon(pos,neg)\n",
    "\tfeatures = []\n",
    "\tfeatures += sample_handling('pos.txt',lexicon,[1,0])\n",
    "\tfeatures += sample_handling('neg.txt',lexicon,[0,1])\n",
    "\trandom.shuffle(features)\n",
    "\tfeatures = np.array(features)\n",
    "\n",
    "\ttesting_size = int(test_size*len(features))\n",
    "\n",
    "\ttrain_x = list(features[:,0][:-testing_size])\n",
    "\ttrain_y = list(features[:,1][:-testing_size])\n",
    "\ttest_x = list(features[:,0][-testing_size:])\n",
    "\ttest_y = list(features[:,1][-testing_size:])\n",
    "\n",
    "\treturn train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1088,
     "status": "error",
     "timestamp": 1539524900710,
     "user": {
      "displayName": "ARUN VERMA",
      "photoUrl": "",
      "userId": "03556269896030076061"
     },
     "user_tz": -330
    },
    "id": "kkyGkGDGuZ7U",
    "outputId": "ad2ea51b-fd13-405b-f323-2f62aba87319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')\n",
    "with open('sentiment_set.pickle','wb') as f:\n",
    "\tpickle.dump([train_x,train_y,test_x,test_y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1453,
     "status": "error",
     "timestamp": 1539524957229,
     "user": {
      "displayName": "ARUN VERMA",
      "photoUrl": "",
      "userId": "03556269896030076061"
     },
     "user_tz": -330
    },
    "id": "ORruBynEvPQL",
    "outputId": "813d3af5-c42f-42d5-a7f6-c55af9b44483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n"
     ]
    }
   ],
   "source": [
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')\n",
    "\n",
    "n_nodes_hl1 = 1500\n",
    "n_nodes_hl2 = 1500\n",
    "n_nodes_hl3 = 1500\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "hm_epochs = 11\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                  'weight':tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "output_layer = {'f_fum':None,\n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "def neural_network_model(data):\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weight']), hidden_3_layer['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weight']) + output_layer['bias']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "\tprediction = neural_network_model(x)\n",
    "\tcost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "    #tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)\n",
    "\toptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(tf.initialize_all_variables())\n",
    "\t    \n",
    "\t\tfor epoch in range(hm_epochs):\n",
    "\t\t\tepoch_loss = 0\n",
    "\t\t\ti=0\n",
    "\t\t\twhile i < len(train_x):\n",
    "\t\t\t\tstart = i\n",
    "\t\t\t\tend = i+batch_size\n",
    "\t\t\t\tbatch_x = np.array(train_x[start:end])\n",
    "\t\t\t\tbatch_y = np.array(train_y[start:end])\n",
    "\n",
    "\t\t\t\t_, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "\t\t\t\t                                              y: batch_y})\n",
    "\t\t\t\tepoch_loss += c\n",
    "\t\t\t\ti+=batch_size\n",
    "\t\t\t\t\n",
    "\t\t\tprint('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\t\tcorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\t\taccuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "\t\tprint('Accuracy:',accuracy.eval({x:test_x, y:test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of 11 loss: 1180991.033203125\n",
      "Epoch 2 completed out of 11 loss: 459027.55517578125\n",
      "Epoch 3 completed out of 11 loss: 393101.6505126953\n",
      "Epoch 4 completed out of 11 loss: 311298.49169921875\n",
      "Epoch 5 completed out of 11 loss: 89757.20860290527\n",
      "Epoch 6 completed out of 11 loss: 34492.49674797058\n",
      "Epoch 7 completed out of 11 loss: 22751.044996500015\n",
      "Epoch 8 completed out of 11 loss: 22250.26286315918\n",
      "Epoch 9 completed out of 11 loss: 20740.59565448761\n",
      "Epoch 10 completed out of 11 loss: 19186.9540514946\n",
      "Epoch 11 completed out of 11 loss: 20453.56983923912\n",
      "Accuracy: 0.5750469\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word_classifier.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
